import os
import json
import pickle
import warnings
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report
from HMM import continueHMM  # tuyá»‡t Ä‘á»‘i

# CÃ¡c import dÃ¹ng Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“ chá»‰ phá»¥c vá»¥ training, trÃ¡nh import khi deploy
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
except Exception:
    plt = None
    sns = None

warnings.filterwarnings("ignore", category=DeprecationWarning)

def _init_params(num_states, seq_list):
    X = np.vstack(seq_list)
    D = X.shape[1]
    pi = np.full(num_states, 1.0/num_states)
    A = np.zeros((num_states, num_states))
    for i in range(num_states):
        stay = 0.6
        move = 0.4
        if i == num_states - 1:
            A[i, i] = 1.0
        else:
            A[i, i] = stay
            A[i, i+1] = move
    A /= A.sum(axis=1, keepdims=True)

    means = np.random.randn(num_states, D)
    global_var = np.var(X, axis=0) + 1e-3
    
    covariances = np.zeros((num_states, D, D))
    for s in range(num_states):
        covariances[s] = np.diag(global_var)

    return A, pi, means, covariances

def train_hmm(X_train, y_train, class_names, num_states=5, n_loop=30, tol=1e-3):
    """
    Huáº¥n luyá»‡n mÃ´ hÃ¬nh HMM
    
    Args:
        X_train: Dá»¯ liá»‡u huáº¥n luyá»‡n
        y_train: NhÃ£n huáº¥n luyá»‡n
        class_names: TÃªn cÃ¡c lá»›p
        num_states: Sá»‘ tráº¡ng thÃ¡i áº©n
        n_loop: Sá»‘ vÃ²ng láº·p tá»‘i Ä‘a
        tol: NgÆ°á»¡ng há»™i tá»¥
    
    Returns:
        models: Dictionary chá»©a cÃ¡c mÃ´ hÃ¬nh HMM Ä‘Ã£ huáº¥n luyá»‡n
    """
    print(f"\n{'='*60}")
    print(f"ğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N MÃ” HÃŒNH HMM")
    print(f"{'='*60}")
    print(f"   - Sá»‘ tráº¡ng thÃ¡i áº©n: {num_states}")
    print(f"   - Sá»‘ vÃ²ng láº·p tá»‘i Ä‘a: {n_loop}")
    print(f"   - NgÆ°á»¡ng há»™i tá»¥: {tol}")
    print(f"{'='*60}\n")
    
    models = []
    for cls_id, cls_name in enumerate(class_names):
        seq_list = [X_train[i] for i, y in enumerate(y_train) if y == cls_id]
        A, pi, means, covs = _init_params(num_states, seq_list)
        model = continueHMM(A=A, means=means, covariances=covs, pi=pi).fit(seq_list, n_loop=n_loop, bound_learning=tol)
        models.append(model)
        print(f"âœ… HoÃ n thÃ nh huáº¥n luyá»‡n lá»›p: {cls_name}\n")
    
    print(f"{'='*60}")
    print(f"âœ… HOÃ€N THÃ€NH HUáº¤N LUYá»†N Táº¤T Cáº¢ CÃC Lá»šP")
    print(f"{'='*60}\n")
    
    return models


def evaluate_hmm(models, X_test, y_test, class_names):
    """
    ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh HMM
    """
    print(f"\n{'='*60}")
    print(f"ğŸ” Báº®T Äáº¦U ÄÃNH GIÃ MÃ” HÃŒNH")
    print(f"{'='*60}\n")
    
    # Dá»± Ä‘oÃ¡n
    y_pred = []
    for seq in X_test:
        scores = [m.forward(seq)[0] for m in models]
        y_pred.append(int(np.argmax(scores)))

    # Metrics chung
    acc = accuracy_score(y_test, y_pred)
    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)
    precision_weighted = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)
    recall_weighted = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)
    f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    
    # Ma tráº­n nháº§m láº«n
    print(f"{'='*60}")
    print(f"ğŸ“Š MA TRáº¬N NHáº¦M LáºªN")
    print(f"{'='*60}")
    cm = confusion_matrix(y_test, y_pred)
    print("\nConfusion Matrix (raw):")
    
    # Váº½ heatmap confusion
    plt.figure(figsize=(max(6, len(class_names)*0.7), max(5, len(class_names)*0.7)))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.show()
    
    # Accuracy tá»«ng lá»›p: Ä‘Ãºng / tá»•ng thá»±c táº¿ lá»›p Ä‘Ã³
    per_class_accuracy = []
    supports = cm.sum(axis=1)
    for i in range(len(class_names)):
        acc_i = (cm[i, i] / supports[i]) if supports[i] > 0 else 0.0
        per_class_accuracy.append(acc_i)
    
    print(f"\n{'='*60}")
    print("ğŸ“Œ ACCURACY Tá»ªNG Lá»šP")
    print(f"{'='*60}")
    print(f"{'Lá»›p':<20} {'Support':>8} {'Correct':>8} {'Acc':>8}")
    for i, cls in enumerate(class_names):
        print(f"{cls:<20} {supports[i]:>8} {cm[i,i]:>8} {per_class_accuracy[i]:>8.2%}")
    
    # BÃ¡o cÃ¡o phÃ¢n loáº¡i
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ BÃO CÃO PHÃ‚N LOáº I CHI TIáº¾T")
    print(f"{'='*60}")
    print(classification_report(y_test, y_pred, target_names=class_names, zero_division=0))
    
    # Tá»•ng káº¿t
    print(f"{'='*60}")
    print(f"ğŸ“Š Tá»”NG Káº¾T Káº¾T QUáº¢")
    print(f"{'='*60}")
    print(f"   - Accuracy (Global):  {acc:.4f}")
    print(f"   - Precision Macro:    {precision_macro:.4f}")
    print(f"   - Precision Weighted: {precision_weighted:.4f}")
    print(f"   - Recall Macro:       {recall_macro:.4f}")
    print(f"   - Recall Weighted:    {recall_weighted:.4f}")
    print(f"   - F1 Macro:           {f1_macro:.4f}")
    print(f"   - F1 Weighted:        {f1_weighted:.4f}")
    print(f"{'='*60}\n")
    
    metrics = {
        'accuracy': acc,
        'precision_macro': precision_macro,
        'precision_weighted': precision_weighted,
        'recall_macro': recall_macro,
        'recall_weighted': recall_weighted,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'confusion_matrix': cm,
        'y_pred': y_pred,
        'per_class_accuracy': np.array(per_class_accuracy)
    }
    return metrics


def train_and_evaluate_continue_hmm(X_train, X_test, y_train, y_test, class_names, 
                                    num_states=5, n_loop=30, tol=1e-3):
    """
    HÃ m káº¿t há»£p huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ (giá»¯ láº¡i Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch ngÆ°á»£c)
    
    Returns:
        models: Dictionary chá»©a cÃ¡c mÃ´ hÃ¬nh HMM
        metrics: Dictionary chá»©a cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡
    """
    # Huáº¥n luyá»‡n
    models = train_hmm(X_train, y_train, class_names, num_states, n_loop, tol)
    # ÄÃ¡nh giÃ¡
    metrics = evaluate_hmm(models, X_test, y_test, class_names)
    return models, metrics

def save_model(models, scaler, metrics, class_names, save_dir='saved_models', model_name=None):
    """
    LÆ°u mÃ´ hÃ¬nh HMM (list), scaler vÃ  metrics.
    
    Args:
        models: List (danh sÃ¡ch) chá»©a cÃ¡c mÃ´ hÃ¬nh HMM Ä‘Ã£ huáº¥n luyá»‡n
        scaler: Scaler Ä‘Ã£ Ä‘Æ°á»£c fit
        metrics: Dictionary chá»©a cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡
        class_names: List (danh sÃ¡ch) tÃªn cÃ¡c lá»›p (vÃ­ dá»¥: ['class_A', 'class_B'])
    """
    # 1. Táº¡o tÃªn mÃ´ hÃ¬nh náº¿u chÆ°a cÃ³
    if model_name is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = f"hmm_model_{timestamp}"
    
    # 2. Táº¡o thÆ° má»¥c lÆ°u
    save_path = os.path.join(save_dir, model_name)
    os.makedirs(save_path, exist_ok=True)
    
    print(f"\n{'='*60}")
    print(f"ğŸ’¾ Báº®T Äáº¦U LÆ¯U MÃ” HÃŒNH")
    print(f"   - ThÆ° má»¥c lÆ°u: {save_path}")
    print(f"{'='*60}\n")
    
    # 1. LÆ°u models (HMM)
    models_path = os.path.join(save_path, 'models.pkl')
    with open(models_path, 'wb') as f:
        pickle.dump(models, f)
    print(f"âœ… ÄÃ£ lÆ°u models (dáº¡ng list) táº¡i: {models_path}")
    
    # 2. LÆ°u scaler
    scaler_path = os.path.join(save_path, 'scaler.pkl')
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"âœ… ÄÃ£ lÆ°u scaler táº¡i: {scaler_path}")

    # 3. Chuyá»ƒn Ä‘á»•i metrics (Numpy -> list) Ä‘á»ƒ lÆ°u JSON
    metrics_serializable = {}
    for key, value in metrics.items():
        if isinstance(value, np.ndarray):
            metrics_serializable[key] = value.tolist()
        elif isinstance(value, (np.int64, np.int32, np.float64, np.float32, np.bool_)):
            metrics_serializable[key] = value.item() # DÃ¹ng .item() an toÃ n hÆ¡n
        else:
            metrics_serializable[key] = value
            
    metrics_path = os.path.join(save_path, 'metrics.json')
    with open(metrics_path, 'w', encoding='utf-8') as f:
        # DÃ¹ng metrics_serializable Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½
        json.dump(metrics_serializable, f, indent=4, ensure_ascii=False) 
    print(f"âœ… ÄÃ£ lÆ°u metrics táº¡i: {metrics_path}")
    
    
    # 4. LÆ°u thÃ´ng tin tÃ³m táº¯t
    summary = {
        'model_name': model_name,
        'save_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'num_classes': len(models),
        'class_names': class_names,  # Láº¥y tá»« tham sá»‘ class_names
        'accuracy': float(metrics.get('accuracy', 0)),
        'f1_macro': float(metrics.get('f1_macro', 0)),
        'f1_weighted': float(metrics.get('f1_weighted', 0))
    }
    
    summary_path = os.path.join(save_path, 'summary.json')
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=4, ensure_ascii=False)
    print(f"âœ… ÄÃ£ lÆ°u summary táº¡i: {summary_path}")
    
    # In káº¿t quáº£ cuá»‘i cÃ¹ng
    print(f"\n{'='*60}")
    print(f"âœ… HOÃ€N THÃ€NH LÆ¯U MÃ” HÃŒNH")
    print(f"{'='*60}")
    print(f"   ğŸ“ ThÆ° má»¥c: {save_path}")
    print(f"   ğŸ“Š Accuracy: {summary['accuracy']:.4f}")
    print(f"   ğŸ“ˆ F1-Score (Macro): {summary['f1_macro']:.4f}")
    print(f"{'='*60}\n")
    
    return save_path


def load_model(load_path):
    """
    Táº£i mÃ´ hÃ¬nh HMM, scaler vÃ  metrics
    
    Args:
        load_path: ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a mÃ´ hÃ¬nh
    
    Returns:
        models: Dictionary chá»©a cÃ¡c mÃ´ hÃ¬nh HMM
        scaler: Scaler
        metrics: Dictionary chá»©a cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡
        summary: Dictionary chá»©a thÃ´ng tin tÃ³m táº¯t
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“‚ Báº®T Äáº¦U Táº¢I MÃ” HÃŒNH")
    print(f"{'='*60}")
    print(f"   - ÄÆ°á»ng dáº«n: {load_path}")
    print(f"{'='*60}\n")
    
    if not os.path.exists(load_path):
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c: {load_path}")

    models_path = os.path.join(load_path, 'models.pkl')
    with open(models_path, 'rb') as f:
        models = pickle.load(f)

    scaler_path = os.path.join(load_path, 'scaler.pkl')
    with open(scaler_path, 'rb') as f:
        scaler = pickle.load(f)

    metrics_path = os.path.join(load_path, 'metrics.json')
    with open(metrics_path, 'r', encoding='utf-8') as f:
        metrics = json.load(f)
    if 'confusion_matrix' in metrics:
        metrics['confusion_matrix'] = np.array(metrics['confusion_matrix'])
    if 'y_pred' in metrics:
        metrics['y_pred'] = np.array(metrics['y_pred'])

    summary_path = os.path.join(load_path, 'summary.json')
    with open(summary_path, 'r', encoding='utf-8') as f:
        summary = json.load(f)

    print(f"\n{'='*60}")
    print(f"âœ… HOÃ€N THÃ€NH Táº¢I MÃ” HÃŒNH")
    print(f"{'='*60}")
    print(f"   ğŸ“… NgÃ y lÆ°u: {summary['save_time']}")
    print(f"   ğŸ·ï¸  Sá»‘ lá»›p: {summary['num_classes']}")
    print(f"   ğŸ“Š Accuracy: {summary['accuracy']:.4f}")
    print(f"   ğŸ“ˆ F1-Score (Macro): {summary['f1_macro']:.4f}")
    print(f"{'='*60}\n")
    
    return models, scaler, metrics, summary

